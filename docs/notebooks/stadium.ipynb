{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ボートレース場ごとの着順予想モデル\n",
    "\n",
    "対象：2025年12月1日～31日のデータを使用して、ボートレース場ごとに着順予想モデルを構築。\n",
    "GradientBoostingRegressor と GradientBoostingClassifier を比較します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ変形関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape functions ready\n"
     ]
    }
   ],
   "source": [
    "def reshape_programs(df):\n",
    "    \"\"\"\n",
    "    Programs を艇単位に変形\n",
    "    各艇について、レース情報と艇固有情報（選手、モーター、ボート）を1行に\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    race_cols = ['レースコード', 'レース日', 'レース場', 'レース回']\n",
    "    \n",
    "    for frame in range(1, 7):\n",
    "        prefix = f'{frame}枠_'\n",
    "        cols = [c for c in df.columns if c.startswith(prefix)]\n",
    "        if cols:\n",
    "            tmp = df[race_cols + cols].copy()\n",
    "            tmp.columns = race_cols + [c[len(prefix):] for c in cols]\n",
    "            tmp['枠'] = frame\n",
    "            frames.append(tmp)\n",
    "    \n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "def reshape_previews(df):\n",
    "    \"\"\"\n",
    "    Previews を艇単位に変形\n",
    "    レース情報と艇固有情報（展示タイム、チルト調整など）を1行に\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    \n",
    "    # レース共通属性を抽出\n",
    "    race_cols = ['レースコード', 'レース日', 'レース場', 'レース回']\n",
    "    race_attrs = ['風速(m)', '風向', '波の高さ(cm)', '天候', '気温(℃)', '水温(℃)']\n",
    "    \n",
    "    # 各艇について処理\n",
    "    for boat in range(1, 7):\n",
    "        prefix = f'艇{boat}_'\n",
    "        boat_cols = [c for c in df.columns if c.startswith(prefix)]\n",
    "        if boat_cols:\n",
    "            tmp = df[race_cols + race_attrs + boat_cols].copy()\n",
    "            # 艇固有列をリネーム\n",
    "            boat_col_names = [c[len(prefix):] for c in boat_cols]\n",
    "            tmp.columns = race_cols + race_attrs + boat_col_names\n",
    "            tmp['艇番'] = boat\n",
    "            frames.append(tmp)\n",
    "    \n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "def reshape_results(df):\n",
    "    \"\"\"\n",
    "    Results を艇単位に変形\n",
    "    着順情報を艇番とマッチングして1行に集約\n",
    "    \"\"\"\n",
    "    # 1着～6着の情報を艇番でマッピング\n",
    "    result_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        race_code = row['レースコード']\n",
    "        \n",
    "        for place in range(1, 7):\n",
    "            boat_col = f'{place}着_艇番'\n",
    "            if boat_col in df.columns and pd.notna(row[boat_col]):\n",
    "                boat_num = int(row[boat_col])\n",
    "                result_list.append({\n",
    "                    'レースコード': race_code,\n",
    "                    '艇番': boat_num,\n",
    "                    '着順': place\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(result_list) if result_list else pd.DataFrame()\n",
    "\n",
    "print('Reshape functions ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2025年12月のデータで学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for 2025-12...\n",
      "✓ 01\n",
      "✓ 02\n",
      "✓ 03\n",
      "✓ 04\n",
      "✓ 05\n",
      "✓ 06\n",
      "✓ 07\n",
      "✓ 08\n",
      "✓ 09\n",
      "✓ 10\n",
      "✓ 11\n",
      "✓ 12\n",
      "✓ 13\n",
      "✓ 14\n",
      "✓ 15\n",
      "✓ 16\n",
      "✓ 17\n",
      "✓ 18\n",
      "✓ 19\n",
      "✓ 20\n",
      "✓ 21\n",
      "✓ 22\n",
      "✓ 23\n",
      "✓ 24\n",
      "✓ 25\n",
      "✓ 26\n",
      "✓ 27\n",
      "✓ 28\n",
      "✓ 29\n",
      "✓ 30\n",
      "✓ 31\n",
      "\n",
      "Loaded 31 days\n"
     ]
    }
   ],
   "source": [
    "# Load data for 2025-12\n",
    "cwd = Path.cwd()\n",
    "repo_root = cwd if (cwd / 'data').exists() else cwd.parent.parent\n",
    "\n",
    "year = '2025'\n",
    "month = '12'\n",
    "date_list = [f'{day:02d}' for day in range(1, 32)]\n",
    "\n",
    "print(f\"Loading data for {year}-{month}...\")\n",
    "\n",
    "all_data = {}\n",
    "for date_str in date_list:\n",
    "    prog_path = repo_root / 'data' / 'programs' / year / month / f'{date_str}.csv'\n",
    "    prev_path = repo_root / 'data' / 'previews' / year / month / f'{date_str}.csv'\n",
    "    res_path = repo_root / 'data' / 'results' / year / month / f'{date_str}.csv'\n",
    "    \n",
    "    if prog_path.exists() and prev_path.exists() and res_path.exists():\n",
    "        all_data[date_str] = {\n",
    "            'programs': pd.read_csv(prog_path),\n",
    "            'previews': pd.read_csv(prev_path),\n",
    "            'results': pd.read_csv(res_path)\n",
    "        }\n",
    "        print(f\"✓ {date_str}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(all_data)} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. データ統合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 01: (432, 47) (features: 39)\n",
      "✓ 02: (360, 47) (features: 39)\n",
      "✓ 03: (360, 47) (features: 39)\n",
      "✓ 04: (576, 47) (features: 39)\n",
      "✓ 05: (648, 47) (features: 39)\n",
      "✓ 06: (576, 47) (features: 39)\n",
      "✓ 07: (504, 47) (features: 39)\n",
      "✓ 08: (504, 47) (features: 39)\n",
      "✓ 09: (660, 47) (features: 39)\n",
      "✓ 10: (642, 47) (features: 39)\n",
      "✓ 11: (504, 47) (features: 39)\n",
      "✓ 12: (504, 47) (features: 39)\n",
      "✓ 13: (648, 47) (features: 39)\n",
      "✓ 14: (720, 47) (features: 39)\n",
      "✓ 15: (660, 47) (features: 39)\n",
      "✓ 16: (498, 47) (features: 39)\n",
      "✓ 17: (564, 47) (features: 39)\n",
      "✓ 18: (516, 47) (features: 39)\n",
      "✓ 19: (492, 47) (features: 39)\n",
      "✓ 20: (552, 47) (features: 39)\n",
      "✓ 21: (504, 47) (features: 39)\n",
      "✓ 22: (720, 47) (features: 39)\n",
      "✓ 23: (720, 47) (features: 39)\n",
      "✓ 24: (1008, 47) (features: 39)\n",
      "✓ 25: (648, 47) (features: 39)\n",
      "✓ 26: (648, 47) (features: 39)\n",
      "✓ 27: (636, 47) (features: 39)\n",
      "✓ 28: (642, 47) (features: 39)\n",
      "✓ 29: (714, 47) (features: 39)\n",
      "✓ 30: (846, 47) (features: 39)\n",
      "✓ 31: (792, 47) (features: 39)\n",
      "\n",
      "Final: (18798, 47)\n",
      "Dates: 31\n",
      "Stadiums: 21\n"
     ]
    }
   ],
   "source": [
    "combined_data = []\n",
    "\n",
    "for date_str, data in all_data.items():\n",
    "    try:\n",
    "        prog = reshape_programs(data['programs'])\n",
    "        prev = reshape_previews(data['previews'])\n",
    "        res = reshape_results(data['results'])\n",
    "        \n",
    "        if prev.empty or prog.empty or res.empty:\n",
    "            print(f'✗ {date_str}: Empty dataframe')\n",
    "            continue\n",
    "        \n",
    "        # Step 1: Merge previews + programs\n",
    "        # レースコードと艇番の両方でマッチング\n",
    "        # 重複する列を事前に処理\n",
    "        prog_cols = set(prog.columns)\n",
    "        prev_cols = set(prev.columns)\n",
    "        overlap_cols = prog_cols & prev_cols - {'レースコード', '艇番'}\n",
    "        \n",
    "        # 重複する列を prog から削除（prev を優先）\n",
    "        prog_to_merge = prog.drop(columns=list(overlap_cols))\n",
    "        \n",
    "        merged = prev.merge(\n",
    "            prog_to_merge,\n",
    "            on=['レースコード', '艇番'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        if merged.empty:\n",
    "            print(f'✗ {date_str}: No match between previews and programs')\n",
    "            continue\n",
    "        \n",
    "        # Step 2: Merge with results\n",
    "        merged = merged.merge(\n",
    "            res[['レースコード', '艇番', '着順']],\n",
    "            on=['レースコード', '艇番'],\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        merged['日付'] = date_str\n",
    "        combined_data.append(merged)\n",
    "        \n",
    "        # Count features (columns not in metadata)\n",
    "        metadata_cols = {'レースコード', 'レース日', 'レース場', 'レース回', '艇番', '枠', '日付', '着順'}\n",
    "        feature_count = len([c for c in merged.columns if c not in metadata_cols])\n",
    "        print(f'✓ {date_str}: {merged.shape} (features: {feature_count})')\n",
    "    except Exception as e:\n",
    "        print(f'✗ {date_str}: {type(e).__name__}: {str(e)[:80]}')\n",
    "\n",
    "if combined_data:\n",
    "    final_data = pd.concat(combined_data, ignore_index=True)\n",
    "    print(f'\\nFinal: {final_data.shape}')\n",
    "    print(f'Dates: {final_data[\"日付\"].nunique()}')\n",
    "    print(f'Stadiums: {final_data[\"レース場\"].nunique()}')\n",
    "else:\n",
    "    print('No data merged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 特徴量準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaN count after filling: 0\n",
      "\n",
      "Features: 37\n",
      "Samples: 18798\n",
      "Stadiums: 21\n",
      "Target missing: 584\n"
     ]
    }
   ],
   "source": [
    "exclude_cols = {'レースコード', 'レース日', 'レース場', 'レース回', '枠', '艇番', '日付', '着順'}\n",
    "# カテゴリカル列を除外\n",
    "categorical_cols = {'風向', '天候'}\n",
    "\n",
    "numeric_cols = []\n",
    "for col in final_data.columns:\n",
    "    if col not in exclude_cols and col not in categorical_cols:\n",
    "        try:\n",
    "            # 実際に数値に変換できるかテスト\n",
    "            pd.to_numeric(final_data[col], errors='coerce')\n",
    "            numeric_cols.append(col)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Handle NaN values - fill with median for each column\n",
    "X = final_data[numeric_cols].copy()\n",
    "\n",
    "# Convert to numeric with coercion (converts non-numeric to NaN)\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "# Fill NaN with median\n",
    "for col in X.columns:\n",
    "    median_val = X[col].median()\n",
    "    if pd.isna(median_val):\n",
    "        # If all values are NaN, use 0\n",
    "        X[col].fillna(0, inplace=True)\n",
    "    else:\n",
    "        X[col].fillna(median_val, inplace=True)\n",
    "\n",
    "# Verify no NaN remains\n",
    "total_nan = X.isna().sum().sum()\n",
    "print(f'Total NaN count after filling: {total_nan}')\n",
    "\n",
    "y = final_data['着順']\n",
    "stadiums = sorted(final_data['レース場'].unique())\n",
    "\n",
    "print(f'\\nFeatures: {len(numeric_cols)}')\n",
    "print(f'Samples: {len(X)}')\n",
    "print(f'Stadiums: {len(stadiums)}')\n",
    "print(f'Target missing: {y.isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. モデル学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed\n",
      "    stadium model       mae  samples       acc\n",
      "0         1   GBR  1.394577      371       NaN\n",
      "1         1   GBC       NaN      371  0.160714\n",
      "2         2   GBR  1.187518     1494       NaN\n",
      "3         2   GBC       NaN     1494  0.242762\n",
      "4         3   GBR  1.158531     1113       NaN\n",
      "5         3   GBC       NaN     1113  0.311377\n",
      "6         4   GBR  1.383277      401       NaN\n",
      "7         4   GBC       NaN      401  0.223140\n",
      "8         5   GBR  1.242250     1354       NaN\n",
      "9         5   GBC       NaN     1354  0.280098\n",
      "10        6   GBR  1.140908     1141       NaN\n",
      "11        6   GBC       NaN     1141  0.268222\n",
      "12        7   GBR  1.160620     1563       NaN\n",
      "13        7   GBC       NaN     1563  0.304904\n",
      "14        8   GBR  1.242461     1476       NaN\n",
      "15        8   GBC       NaN     1476  0.270880\n",
      "16        9   GBR  1.177126     1241       NaN\n",
      "17        9   GBC       NaN     1241  0.268097\n",
      "18       10   GBR  1.150523     1106       NaN\n",
      "19       10   GBC       NaN     1106  0.310241\n",
      "20       11   GBR  1.321237     1276       NaN\n",
      "21       11   GBC       NaN     1276  0.221932\n",
      "22       14   GBR  1.343380      620       NaN\n",
      "23       14   GBC       NaN      620  0.225806\n",
      "24       15   GBR  1.494836      566       NaN\n",
      "25       15   GBC       NaN      566  0.176471\n",
      "26       16   GBR  1.371891      350       NaN\n",
      "27       16   GBC       NaN      350  0.200000\n",
      "28       17   GBR  1.327264      495       NaN\n",
      "29       17   GBC       NaN      495  0.234899\n",
      "30       18   GBR  1.303448      634       NaN\n",
      "31       18   GBC       NaN      634  0.261780\n",
      "32       19   GBR  1.159269      498       NaN\n",
      "33       19   GBC       NaN      498  0.233333\n",
      "34       20   GBR  1.327602      645       NaN\n",
      "35       20   GBC       NaN      645  0.185567\n",
      "36       21   GBR  1.390342      413       NaN\n",
      "37       21   GBC       NaN      413  0.233871\n",
      "38       22   GBR  1.496709      265       NaN\n",
      "39       22   GBC       NaN      265  0.212500\n",
      "40       24   GBR  1.170692     1192       NaN\n",
      "41       24   GBC       NaN     1192  0.326816\n"
     ]
    }
   ],
   "source": [
    "results_summary = []\n",
    "errors = []\n",
    "\n",
    "for stadium in stadiums:\n",
    "    mask = final_data['レース場'] == stadium\n",
    "    X_std = X[mask].reset_index(drop=True)\n",
    "    y_std = y[mask].reset_index(drop=True)\n",
    "    \n",
    "    # Remove missing targets\n",
    "    valid = y_std.notna()\n",
    "    X_std = X_std[valid].reset_index(drop=True)\n",
    "    y_std = y_std[valid].reset_index(drop=True)\n",
    "    \n",
    "    if len(X_std) < 10:\n",
    "        print(f'{stadium}: insufficient data ({len(X_std)} samples)')\n",
    "        continue\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_std, y_std, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "    \n",
    "    # GBR\n",
    "    try:\n",
    "        gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "        gbr.fit(X_train_s, y_train)\n",
    "        mae = mean_absolute_error(y_test, gbr.predict(X_test_s))\n",
    "        results_summary.append({'stadium': stadium, 'model': 'GBR', 'mae': mae, 'samples': len(X_std)})\n",
    "    except Exception as e:\n",
    "        errors.append(f'Stadium {stadium} GBR: {type(e).__name__}: {str(e)[:50]}')\n",
    "    \n",
    "    # GBC\n",
    "    try:\n",
    "        gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "        gbc.fit(X_train_s, y_train)\n",
    "        acc = accuracy_score(y_test, gbc.predict(X_test_s))\n",
    "        results_summary.append({'stadium': stadium, 'model': 'GBC', 'acc': acc, 'samples': len(X_std)})\n",
    "    except Exception as e:\n",
    "        errors.append(f'Stadium {stadium} GBC: {type(e).__name__}: {str(e)[:50]}')\n",
    "\n",
    "if results_summary:\n",
    "    results_df = pd.DataFrame(results_summary)\n",
    "    print('Training completed')\n",
    "    print(results_df)\n",
    "else:\n",
    "    print('No results')\n",
    "    \n",
    "if errors:\n",
    "    print('\\nErrors:')\n",
    "    for err in errors:\n",
    "        print(f'  {err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 21 models to /Users/mahiguch/dev/boatrace/data/models/stadium_models.pkl\n"
     ]
    }
   ],
   "source": [
    "# Store models and scaler info\n",
    "models_dict = {}\n",
    "\n",
    "for stadium in stadiums:\n",
    "    mask = final_data['レース場'] == stadium\n",
    "    X_std = X[mask].reset_index(drop=True)\n",
    "    y_std = y[mask].reset_index(drop=True)\n",
    "    \n",
    "    # Remove missing targets\n",
    "    valid = y_std.notna()\n",
    "    X_std = X_std[valid].reset_index(drop=True)\n",
    "    y_std = y_std[valid].reset_index(drop=True)\n",
    "    \n",
    "    if len(X_std) < 10:\n",
    "        continue\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_std, y_std, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s = scaler.transform(X_test)\n",
    "    \n",
    "    # Train and save GBC model\n",
    "    gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    gbc.fit(X_train_s, y_train)\n",
    "    \n",
    "    models_dict[stadium] = {\n",
    "        'model': gbc,\n",
    "        'scaler': scaler,\n",
    "        'features': numeric_cols\n",
    "    }\n",
    "\n",
    "# Save models\n",
    "model_save_path = repo_root / 'models' / 'stadium_models.pkl'\n",
    "model_save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(model_save_path, 'wb') as f:\n",
    "    pickle.dump(models_dict, f)\n",
    "\n",
    "print(f'Saved {len(models_dict)} models to {model_save_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2026年1月1日のデータで予想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for 2026-01-01:\n",
      "  Programs: (154, 177)\n",
      "  Previews: (118, 53)\n",
      "  Results: (153, 100)\n"
     ]
    }
   ],
   "source": [
    "# Load data for 2026-01-01\n",
    "test_date = '2026-01-01'\n",
    "year_test = '2026'\n",
    "month_test = '01'\n",
    "day_test = '01'\n",
    "\n",
    "prog_path = repo_root / 'data' / 'programs' / year_test / month_test / f'{day_test}.csv'\n",
    "prev_path = repo_root / 'data' / 'previews' / year_test / month_test / f'{day_test}.csv'\n",
    "res_path = repo_root / 'data' / 'results' / year_test / month_test / f'{day_test}.csv'\n",
    "\n",
    "# Check if files exist\n",
    "if not all([prog_path.exists(), prev_path.exists(), res_path.exists()]):\n",
    "    print(f'Missing files for {test_date}:')\n",
    "    print(f'  programs: {prog_path.exists()}')\n",
    "    print(f'  previews: {prev_path.exists()}')\n",
    "    print(f'  results: {res_path.exists()}')\n",
    "else:\n",
    "    prog_test = pd.read_csv(prog_path)\n",
    "    prev_test = pd.read_csv(prev_path)\n",
    "    res_test = pd.read_csv(res_path)\n",
    "    \n",
    "    print(f'Loaded data for {test_date}:')\n",
    "    print(f'  Programs: {prog_test.shape}')\n",
    "    print(f'  Previews: {prev_test.shape}')\n",
    "    print(f'  Results: {res_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. データ変形とマージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped:\n",
      "  Programs: (924, 33)\n",
      "  Previews: (708, 17)\n",
      "  Results: (900, 3)\n",
      "\n",
      "Debug columns:\n",
      "  \"レース場\" in previews: True\n",
      "  \"レース場\" in programs: True\n",
      "  Overlapping columns (to remove from programs): {'レース回', 'レース場', 'レース日'}\n",
      "\n",
      "Merged test data: (708, 46)\n",
      "Contains レース場: True\n",
      "Unique stadiums: 10\n",
      "Actual results available: 687 rows\n"
     ]
    }
   ],
   "source": [
    "# Reshape and merge test data\n",
    "prog_reshaped = reshape_programs(prog_test)\n",
    "prev_reshaped = reshape_previews(prev_test)\n",
    "res_reshaped = reshape_results(res_test)\n",
    "\n",
    "print(f'Reshaped:')\n",
    "print(f'  Programs: {prog_reshaped.shape}')\n",
    "print(f'  Previews: {prev_reshaped.shape}')\n",
    "print(f'  Results: {res_reshaped.shape}')\n",
    "\n",
    "# Debug: Check columns\n",
    "print(f'\\nDebug columns:')\n",
    "print(f'  \"レース場\" in previews: {\"レース場\" in prev_reshaped.columns}')\n",
    "print(f'  \"レース場\" in programs: {\"レース場\" in prog_reshaped.columns}')\n",
    "\n",
    "# Handle overlapping columns before merge\n",
    "prog_cols = set(prog_reshaped.columns)\n",
    "prev_cols = set(prev_reshaped.columns)\n",
    "overlap_cols = (prog_cols & prev_cols) - {'レースコード', '艇番'}\n",
    "\n",
    "print(f'  Overlapping columns (to remove from programs): {overlap_cols}')\n",
    "\n",
    "# Remove overlapping columns from programs (keep previews version)\n",
    "prog_to_merge = prog_reshaped.drop(columns=list(overlap_cols))\n",
    "\n",
    "# Merge without suffixes since we removed overlap\n",
    "test_data = prev_reshaped.merge(\n",
    "    prog_to_merge,\n",
    "    on=['レースコード', '艇番'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "test_data = test_data.merge(\n",
    "    res_reshaped[['レースコード', '艇番', '着順']],\n",
    "    on=['レースコード', '艇番'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Verify columns\n",
    "print(f'\\nMerged test data: {test_data.shape}')\n",
    "print(f'Contains レース場: {\"レース場\" in test_data.columns}')\n",
    "if 'レース場' in test_data.columns:\n",
    "    print(f'Unique stadiums: {test_data[\"レース場\"].nunique()}')\n",
    "print(f'Actual results available: {test_data[\"着順\"].notna().sum()} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 予想実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 21 models\n",
      "\n",
      "Sanrentan predictions completed: 708 rows\n",
      "Sample predictions:\n",
      "         レースコード  艇番      予想三連単\n",
      "0  202601010601   1  (1, 2, 6)\n",
      "1  202601010602   1  (2, 5, 1)\n",
      "2  202601010603   1  (1, 2, 3)\n",
      "3  202601010604   1  (1, 3, 4)\n",
      "4  202601010605   1  (6, 2, 1)\n",
      "5  202601010606   1  (1, 3, 2)\n",
      "6  202601010607   1  (2, 1, 3)\n",
      "7  202601010608   1  (1, 2, 3)\n",
      "8  202601010609   1  (1, 2, 3)\n",
      "9  202601010610   1  (1, 2, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load saved models\n",
    "with open(model_save_path, 'rb') as f:\n",
    "    models_dict = pickle.load(f)\n",
    "\n",
    "print(f'Loaded {len(models_dict)} models')\n",
    "\n",
    "# Prepare features for prediction\n",
    "X_test_pred = test_data[numeric_cols].copy()\n",
    "for col in X_test_pred.columns:\n",
    "    X_test_pred[col] = pd.to_numeric(X_test_pred[col], errors='coerce')\n",
    "\n",
    "for col in X_test_pred.columns:\n",
    "    median_val = X_test_pred[col].median()\n",
    "    if pd.isna(median_val):\n",
    "        X_test_pred[col].fillna(0, inplace=True)\n",
    "    else:\n",
    "        X_test_pred[col].fillna(median_val, inplace=True)\n",
    "\n",
    "# Function to generate Sanrentan prediction from probabilities\n",
    "def predict_sanrentan(model, scaler, X_row, stadiums_set):\n",
    "    \"\"\"\n",
    "    各艇の確率を計算して、上位3艇を選び三連単を生成\n",
    "    \"\"\"\n",
    "    X_scaled = scaler.transform(X_row)\n",
    "    \n",
    "    # predict_proba で各着順の確率を取得\n",
    "    proba = model.predict_proba(X_scaled)[0]\n",
    "    classes = model.classes_\n",
    "    \n",
    "    # 確率をソート\n",
    "    prob_dict = {cls: prob for cls, prob in zip(classes, proba)}\n",
    "    sorted_probs = sorted(prob_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 上位3つの着順を抽出\n",
    "    top_3 = sorted_probs[:3]\n",
    "    \n",
    "    return top_3\n",
    "\n",
    "# Make predictions for Sanrentan\n",
    "sanrentan_predictions = []\n",
    "\n",
    "for idx, row in test_data.iterrows():\n",
    "    stadium = row['レース場']\n",
    "    boat_num = row['艇番']\n",
    "    \n",
    "    # Skip if no model for this stadium\n",
    "    if stadium not in models_dict:\n",
    "        sanrentan_predictions.append(None)\n",
    "        continue\n",
    "    \n",
    "    model_info = models_dict[stadium]\n",
    "    model = model_info['model']\n",
    "    scaler = model_info['scaler']\n",
    "    \n",
    "    # Prepare features\n",
    "    X_row = X_test_pred.iloc[idx:idx+1]\n",
    "    \n",
    "    try:\n",
    "        top_3 = predict_sanrentan(model, scaler, X_row, stadium)\n",
    "        # 艇番を確率でソート\n",
    "        top_boats = [int(boat) for boat, _ in top_3]\n",
    "        sanrentan_predictions.append(tuple(top_boats))\n",
    "    except:\n",
    "        sanrentan_predictions.append(None)\n",
    "\n",
    "test_data['予想三連単'] = sanrentan_predictions\n",
    "\n",
    "print(f'\\nSanrentan predictions completed: {test_data[\"予想三連単\"].notna().sum()} rows')\n",
    "print(f'Sample predictions:')\n",
    "print(test_data[['レースコード', '艇番', '予想三連単']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 的中率の計算（三連単）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race-level Sanrentan results: 116 races\n",
      "Race-level predictions: 118 races\n",
      "\n",
      "三連単的中率: 5/116 = 4.31%\n",
      "\n",
      "結果詳細:\n",
      "      レースコード     予想三連単     実績三連単    的中\n",
      "202601010601 (1, 2, 6) (1, 3, 2) False\n",
      "202601010602 (2, 5, 1) (5, 4, 3) False\n",
      "202601010603 (1, 2, 3) (1, 3, 2) False\n",
      "202601010604 (1, 3, 4) (2, 5, 3) False\n",
      "202601010605 (6, 2, 1) (2, 3, 6) False\n",
      "202601010606 (1, 3, 2) (1, 6, 5) False\n",
      "202601010607 (2, 1, 3) (2, 1, 3)  True\n",
      "202601010608 (1, 2, 3) (4, 2, 1) False\n",
      "202601010609 (1, 2, 3) (5, 2, 3) False\n",
      "202601010610 (1, 2, 5) (1, 2, 5)  True\n",
      "202601010611 (1, 3, 2) (1, 2, 3) False\n",
      "202601010612 (1, 2, 3) (3, 5, 1) False\n",
      "202601010701 (1, 2, 3) (1, 2, 3)  True\n",
      "202601010702 (1, 4, 3) (1, 2, 4) False\n",
      "202601010703 (3, 1, 5) (4, 6, 3) False\n",
      "202601010704 (2, 4, 3) (5, 6, 3) False\n",
      "202601010705 (5, 1, 6) (6, 5, 4) False\n",
      "202601010706 (1, 2, 4) (1, 5, 3) False\n",
      "202601010707 (1, 2, 4) (1, 4, 3) False\n",
      "202601010708 (1, 4, 3) (1, 2, 6) False\n",
      "202601010709 (1, 2, 4) (5, 4, 3) False\n",
      "202601010710 (3, 1, 4) (1, 6, 3) False\n",
      "202601010711 (1, 3, 2) (3, 1, 4) False\n",
      "202601010712 (1, 3, 6) (1, 3, 2) False\n",
      "202601010801 (1, 3, 2) (1, 3, 4) False\n",
      "202601010802 (1, 4, 2) (1, 4, 3) False\n",
      "202601010803 (1, 5, 2) (1, 3, 2) False\n",
      "202601010804 (1, 2, 4) (1, 3, 2) False\n",
      "202601010805 (5, 4, 1) (5, 1, 4) False\n",
      "202601010806 (1, 2, 3) (1, 3, 4) False\n",
      "202601010807 (1, 2, 5) (1, 2, 5)  True\n",
      "202601010808 (2, 1, 5) (4, 3, 5) False\n",
      "202601010809 (1, 2, 3) (2, 3, 6) False\n",
      "202601010810 (1, 2, 3) (3, 5, 2) False\n",
      "202601010811 (1, 2, 3) (1, 4, 2) False\n",
      "202601010812 (2, 1, 5) (1, 3, 4) False\n",
      "202601010901 (1, 2, 3) (1, 3, 6) False\n",
      "202601010902 (1, 2, 3) (3, 1, 2) False\n",
      "202601010903 (5, 6, 2) (4, 3, 5) False\n",
      "202601010904 (5, 3, 6) (2, 1, 4) False\n",
      "202601010906 (6, 1, 2) (4, 3, 2) False\n",
      "202601010907 (1, 3, 2) (4, 3, 5) False\n",
      "202601010908 (2, 1, 3) (4, 1, 6) False\n",
      "202601010909 (1, 2, 3) (1, 2, 4) False\n",
      "202601010910 (3, 4, 1) (2, 4, 3) False\n",
      "202601010911 (1, 3, 6) (3, 1, 5) False\n",
      "202601010912 (1, 3, 2) (1, 3, 2)  True\n",
      "202601011001 (1, 2, 3) (1, 3, 2) False\n",
      "202601011002 (1, 2, 4) (2, 1, 5) False\n",
      "202601011003 (1, 2, 3) (1, 3, 5) False\n",
      "202601011004 (1, 2, 3) (1, 3, 4) False\n",
      "202601011005 (1, 2, 3) (1, 5, 3) False\n",
      "202601011006 (6, 1, 3) (3, 6, 5) False\n",
      "202601011007 (1, 3, 2) (1, 5, 3) False\n",
      "202601011008 (1, 4, 5) (3, 4, 6) False\n",
      "202601011009 (1, 4, 3) (5, 3, 6) False\n",
      "202601011010 (1, 3, 2) (1, 2, 5) False\n",
      "202601011011 (2, 1, 3) (2, 3, 4) False\n",
      "202601011012 (2, 1, 6) (4, 2, 6) False\n",
      "202601011501 (6, 1, 3) (1, 3, 2) False\n",
      "202601011502 (1, 4, 3) (1, 4, 2) False\n",
      "202601011503 (6, 4, 1) (3, 2, 4) False\n",
      "202601011504 (1, 3, 6) (5, 1, 6) False\n",
      "202601011505 (6, 2, 1) (4, 6, 1) False\n",
      "202601011506 (1, 4, 6) (3, 1, 2) False\n",
      "202601011507 (1, 2, 4) (3, 2, 1) False\n",
      "202601011508 (5, 1, 4) (1, 3, 2) False\n",
      "202601011509 (1, 4, 3) (1, 5, 2) False\n",
      "202601011510 (1, 2, 3) (1, 2, 5) False\n",
      "202601011511 (1, 4, 6) (1, 2, 5) False\n",
      "202601011512 (3, 6, 1) (1, 4, 5) False\n",
      "202601011601 (1, 2, 5) (4, 3, 1) False\n",
      "202601011602 (6, 4, 3) (4, 5, 3) False\n",
      "202601011603 (4, 2, 1) (5, 1, 6) False\n",
      "202601011604 (5, 3, 4) (1, 3, 6) False\n",
      "202601011605 (4, 5, 2) (1, 3, 5) False\n",
      "202601011606 (2, 1, 4) (1, 4, 6) False\n",
      "202601011607 (2, 3, 5) (1, 6, 3) False\n",
      "202601011608 (1, 2, 5) (1, 2, 4) False\n",
      "202601011609 (1, 3, 5) (4, 6, 3) False\n",
      "202601011610 (1, 2, 5) (1, 4, 5) False\n",
      "202601011611 (1, 2, 5) (6, 2, 4) False\n",
      "202601011612 (1, 2, 5) (1, 2, 3) False\n",
      "202601011701 (3, 2, 4) (1, 6, 3) False\n",
      "202601011702 (1, 3, 2) (3, 1, 5) False\n",
      "202601011703 (3, 1, 2) (1, 4, 2) False\n",
      "202601011704 (1, 4, 5) (3, 4, 2) False\n",
      "202601011705 (2, 1, 3) (1, 3, 5) False\n",
      "202601011706 (3, 4, 1) (1, 4, 2) False\n",
      "202601011707 (3, 1, 2) (2, 4, 3) False\n",
      "202601011708 (1, 3, 2) (1, 3, 4) False\n",
      "202601011709 (1, 3, 2) (4, 3, 1) False\n",
      "202601011710 (3, 1, 4) (4, 6, 2) False\n",
      "202601011711 (1, 3, 2) (1, 3, 6) False\n",
      "202601011712 (1, 3, 2) (1, 4, 6) False\n",
      "202601012001 (1, 6, 4) (4, 6, 2) False\n",
      "202601012002 (4, 2, 3) (1, 6, 3) False\n",
      "202601012003 (1, 3, 2) (3, 1, 4) False\n",
      "202601012004 (1, 3, 4) (1, 6, 2) False\n",
      "202601012005 (2, 1, 4) (1, 2, 4) False\n",
      "202601012006 (1, 6, 2) (1, 3, 5) False\n",
      "202601012007 (2, 1, 3) (2, 5, 4) False\n",
      "202601012008 (1, 3, 2) (1, 2, 3) False\n",
      "202601012009 (6, 1, 4) (1, 4, 3) False\n",
      "202601012010 (1, 5, 6) (3, 1, 5) False\n",
      "202601012011 (1, 2, 4) (1, 4, 6) False\n",
      "202601012012 (1, 2, 3) (1, 4, 3) False\n",
      "202601012101 (1, 3, 2) (1, 3, 4) False\n",
      "202601012102 (1, 2, 5) (1, 4, 3) False\n",
      "202601012103 (1, 5, 2) (4, 1, 2) False\n",
      "202601012104 (4, 1, 2) (3, 1, 4) False\n",
      "202601012105 (1, 3, 2) (1, 2, 5) False\n",
      "202601012106 (1, 2, 4) (2, 4, 3) False\n",
      "202601012109 (1, 4, 3) (1, 3, 2) False\n",
      "202601012111 (3, 1, 2) (1, 3, 5) False\n",
      "202601012112 (1, 5, 2) (1, 2, 3) False\n"
     ]
    }
   ],
   "source": [
    "# レースコード単位で三連単の着順を取得\n",
    "def get_actual_sanrentan(race_results):\n",
    "    \"\"\"\n",
    "    レースの結果から実際の三連単を取得\n",
    "    race_results: レースの1着～3着の艇番\n",
    "    \"\"\"\n",
    "    if len(race_results) >= 3:\n",
    "        return tuple(race_results[:3])\n",
    "    return None\n",
    "\n",
    "# レースコードごとに実績三連単を計算\n",
    "race_actual_sanrentan = {}\n",
    "for race_code in test_data['レースコード'].unique():\n",
    "    race_mask = test_data['レースコード'] == race_code\n",
    "    race_subset = test_data[race_mask].sort_values('着順')\n",
    "    \n",
    "    # 1着～3着の艇番を取得\n",
    "    sanrentan = tuple(race_subset[race_subset['着順'].notna()].head(3)['艇番'].astype(int).values)\n",
    "    if len(sanrentan) == 3:\n",
    "        race_actual_sanrentan[race_code] = sanrentan\n",
    "\n",
    "print(f'Race-level Sanrentan results: {len(race_actual_sanrentan)} races')\n",
    "\n",
    "# レース単位での予想を集計\n",
    "race_predictions = {}\n",
    "for race_code in test_data['レースコード'].unique():\n",
    "    race_mask = test_data['レースコード'] == race_code\n",
    "    race_subset = test_data[race_mask]\n",
    "    \n",
    "    # 最初の行から予想三連単を取得（すべての艇で同じレースなので最初の行でよい）\n",
    "    if race_subset['予想三連単'].notna().any():\n",
    "        race_predictions[race_code] = race_subset['予想三連単'].iloc[0]\n",
    "\n",
    "print(f'Race-level predictions: {len(race_predictions)} races')\n",
    "\n",
    "# 的中判定\n",
    "sanrentan_matches = []\n",
    "for race_code in race_actual_sanrentan.keys():\n",
    "    if race_code in race_predictions:\n",
    "        actual = race_actual_sanrentan[race_code]\n",
    "        predicted = race_predictions[race_code]\n",
    "        \n",
    "        is_match = (predicted == actual)\n",
    "        sanrentan_matches.append({\n",
    "            'レースコード': race_code,\n",
    "            '予想三連単': predicted,\n",
    "            '実績三連単': actual,\n",
    "            '的中': is_match\n",
    "        })\n",
    "\n",
    "if sanrentan_matches:\n",
    "    sanrentan_df = pd.DataFrame(sanrentan_matches)\n",
    "    correct = sanrentan_df['的中'].sum()\n",
    "    total = len(sanrentan_df)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    print(f'\\n三連単的中率: {correct}/{total} = {accuracy:.2%}')\n",
    "    print(f'\\n結果詳細:')\n",
    "    print(sanrentan_df.to_string(index=False))\n",
    "else:\n",
    "    print('的中判定可能なデータなし')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to: /Users/mahiguch/dev/boatrace/data/data/estimate/2026/01/01.csv\n",
      "Total predictions: 118\n",
      "\n",
      "Sample output:\n",
      "         レースコード  予想1着  予想2着  予想3着\n",
      "0  202601010601     1     2     6\n",
      "1  202601010602     2     5     1\n",
      "2  202601010603     1     2     3\n",
      "3  202601010604     1     3     4\n",
      "4  202601010605     6     2     1\n",
      "5  202601010606     1     3     2\n",
      "6  202601010607     2     1     3\n",
      "7  202601010608     1     2     3\n",
      "8  202601010609     1     2     3\n",
      "9  202601010610     1     2     5\n"
     ]
    }
   ],
   "source": [
    "### 5. 推定結果を CSV に出力\n",
    "\n",
    "# レース単位で三連単の予想を整形して出力\n",
    "output_records = []\n",
    "\n",
    "for race_code in sorted(race_predictions.keys()):\n",
    "    predicted_sanrentan = race_predictions[race_code]\n",
    "    if predicted_sanrentan is not None:\n",
    "        output_records.append({\n",
    "            'レースコード': race_code,\n",
    "            '予想1着': predicted_sanrentan[0],\n",
    "            '予想2着': predicted_sanrentan[1],\n",
    "            '予想3着': predicted_sanrentan[2]\n",
    "        })\n",
    "\n",
    "if output_records:\n",
    "    output_df = pd.DataFrame(output_records)\n",
    "    \n",
    "    # 出力ディレクトリを作成\n",
    "    output_dir = repo_root / 'data' / 'estimate' / year_test / month_test\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # ファイル名\n",
    "    output_path = output_dir / f'{day_test}.csv'\n",
    "    \n",
    "    # CSV に出力\n",
    "    output_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f'Output saved to: {output_path}')\n",
    "    print(f'Total predictions: {len(output_df)}')\n",
    "    print(f'\\nSample output:')\n",
    "    print(output_df.head(10))\n",
    "else:\n",
    "    print('No predictions to output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 的中率レポート ===\n",
      "レース数: 116\n",
      "1着的中: 51/116 (44.0%)\n",
      "2着的中: 30/116 (25.9%)\n",
      "3着的中: 21/116 (18.1%)\n",
      "三連単的中: 5/116 (4.3%)\n",
      "\n",
      "=== 詳細結果 ===\n",
      "      レースコード  予想1着  予想2着  予想3着  実際1着  実際2着  実際3着 1着的中 2着的中 3着的中 全的中\n",
      "202601010601     1     2     6     1     3     2    ○    ×    ×   ×\n",
      "202601010602     2     5     1     5     4     3    ×    ×    ×   ×\n",
      "202601010603     1     2     3     1     3     2    ○    ×    ×   ×\n",
      "202601010604     1     3     4     2     5     3    ×    ×    ×   ×\n",
      "202601010605     6     2     1     2     3     6    ×    ×    ×   ×\n",
      "202601010606     1     3     2     1     6     5    ○    ×    ×   ×\n",
      "202601010607     2     1     3     2     1     3    ○    ○    ○   ○\n",
      "202601010608     1     2     3     4     2     1    ×    ○    ×   ×\n",
      "202601010609     1     2     3     5     2     3    ×    ○    ○   ×\n",
      "202601010610     1     2     5     1     2     5    ○    ○    ○   ○\n",
      "202601010611     1     3     2     1     2     3    ○    ×    ×   ×\n",
      "202601010612     1     2     3     3     5     1    ×    ×    ×   ×\n",
      "202601010701     1     2     3     1     2     3    ○    ○    ○   ○\n",
      "202601010702     1     4     3     1     2     4    ○    ×    ×   ×\n",
      "202601010703     3     1     5     4     6     3    ×    ×    ×   ×\n",
      "202601010704     2     4     3     5     6     3    ×    ×    ○   ×\n",
      "202601010705     5     1     6     6     5     4    ×    ×    ×   ×\n",
      "202601010706     1     2     4     1     5     3    ○    ×    ×   ×\n",
      "202601010707     1     2     4     1     4     3    ○    ×    ×   ×\n",
      "202601010708     1     4     3     1     2     6    ○    ×    ×   ×\n",
      "202601010709     1     2     4     5     4     3    ×    ×    ×   ×\n",
      "202601010710     3     1     4     1     6     3    ×    ×    ×   ×\n",
      "202601010711     1     3     2     3     1     4    ×    ×    ×   ×\n",
      "202601010712     1     3     6     1     3     2    ○    ○    ×   ×\n",
      "202601010801     1     3     2     1     3     4    ○    ○    ×   ×\n",
      "202601010802     1     4     2     1     4     3    ○    ○    ×   ×\n",
      "202601010803     1     5     2     1     3     2    ○    ×    ○   ×\n",
      "202601010804     1     2     4     1     3     2    ○    ×    ×   ×\n",
      "202601010805     5     4     1     5     1     4    ○    ×    ×   ×\n",
      "202601010806     1     2     3     1     3     4    ○    ×    ×   ×\n",
      "202601010807     1     2     5     1     2     5    ○    ○    ○   ○\n",
      "202601010808     2     1     5     4     3     5    ×    ×    ○   ×\n",
      "202601010809     1     2     3     2     3     6    ×    ×    ×   ×\n",
      "202601010810     1     2     3     3     5     2    ×    ×    ×   ×\n",
      "202601010811     1     2     3     1     4     2    ○    ×    ×   ×\n",
      "202601010812     2     1     5     1     3     4    ×    ×    ×   ×\n",
      "202601010901     1     2     3     1     3     6    ○    ×    ×   ×\n",
      "202601010902     1     2     3     3     1     2    ×    ×    ×   ×\n",
      "202601010903     5     6     2     4     3     5    ×    ×    ×   ×\n",
      "202601010904     5     3     6     2     1     4    ×    ×    ×   ×\n",
      "202601010906     6     1     2     4     3     2    ×    ×    ○   ×\n",
      "202601010907     1     3     2     4     3     5    ×    ○    ×   ×\n",
      "202601010908     2     1     3     4     1     6    ×    ○    ×   ×\n",
      "202601010909     1     2     3     1     2     4    ○    ○    ×   ×\n",
      "202601010910     3     4     1     2     4     3    ×    ○    ×   ×\n",
      "202601010911     1     3     6     3     1     5    ×    ×    ×   ×\n",
      "202601010912     1     3     2     1     3     2    ○    ○    ○   ○\n",
      "202601011001     1     2     3     1     3     2    ○    ×    ×   ×\n",
      "202601011002     1     2     4     2     1     5    ×    ×    ×   ×\n",
      "202601011003     1     2     3     1     3     5    ○    ×    ×   ×\n",
      "202601011004     1     2     3     1     3     4    ○    ×    ×   ×\n",
      "202601011005     1     2     3     1     5     3    ○    ×    ○   ×\n",
      "202601011006     6     1     3     3     6     5    ×    ×    ×   ×\n",
      "202601011007     1     3     2     1     5     3    ○    ×    ×   ×\n",
      "202601011008     1     4     5     3     4     6    ×    ○    ×   ×\n",
      "202601011009     1     4     3     5     3     6    ×    ×    ×   ×\n",
      "202601011010     1     3     2     1     2     5    ○    ×    ×   ×\n",
      "202601011011     2     1     3     2     3     4    ○    ×    ×   ×\n",
      "202601011012     2     1     6     4     2     6    ×    ×    ○   ×\n",
      "202601011501     6     1     3     1     3     2    ×    ×    ×   ×\n",
      "202601011502     1     4     3     1     4     2    ○    ○    ×   ×\n",
      "202601011503     6     4     1     3     2     4    ×    ×    ×   ×\n",
      "202601011504     1     3     6     5     1     6    ×    ×    ○   ×\n",
      "202601011505     6     2     1     4     6     1    ×    ×    ○   ×\n",
      "202601011506     1     4     6     3     1     2    ×    ×    ×   ×\n",
      "202601011507     1     2     4     3     2     1    ×    ○    ×   ×\n",
      "202601011508     5     1     4     1     3     2    ×    ×    ×   ×\n",
      "202601011509     1     4     3     1     5     2    ○    ×    ×   ×\n",
      "202601011510     1     2     3     1     2     5    ○    ○    ×   ×\n",
      "202601011511     1     4     6     1     2     5    ○    ×    ×   ×\n",
      "202601011512     3     6     1     1     4     5    ×    ×    ×   ×\n",
      "202601011601     1     2     5     4     3     1    ×    ×    ×   ×\n",
      "202601011602     6     4     3     4     5     3    ×    ×    ○   ×\n",
      "202601011603     4     2     1     5     1     6    ×    ×    ×   ×\n",
      "202601011604     5     3     4     1     3     6    ×    ○    ×   ×\n",
      "202601011605     4     5     2     1     3     5    ×    ×    ×   ×\n",
      "202601011606     2     1     4     1     4     6    ×    ×    ×   ×\n",
      "202601011607     2     3     5     1     6     3    ×    ×    ×   ×\n",
      "202601011608     1     2     5     1     2     4    ○    ○    ×   ×\n",
      "202601011609     1     3     5     4     6     3    ×    ×    ×   ×\n",
      "202601011610     1     2     5     1     4     5    ○    ×    ○   ×\n",
      "202601011611     1     2     5     6     2     4    ×    ○    ×   ×\n",
      "202601011612     1     2     5     1     2     3    ○    ○    ×   ×\n",
      "202601011701     3     2     4     1     6     3    ×    ×    ×   ×\n",
      "202601011702     1     3     2     3     1     5    ×    ×    ×   ×\n",
      "202601011703     3     1     2     1     4     2    ×    ×    ○   ×\n",
      "202601011704     1     4     5     3     4     2    ×    ○    ×   ×\n",
      "202601011705     2     1     3     1     3     5    ×    ×    ×   ×\n",
      "202601011706     3     4     1     1     4     2    ×    ○    ×   ×\n",
      "202601011707     3     1     2     2     4     3    ×    ×    ×   ×\n",
      "202601011708     1     3     2     1     3     4    ○    ○    ×   ×\n",
      "202601011709     1     3     2     4     3     1    ×    ○    ×   ×\n",
      "202601011710     3     1     4     4     6     2    ×    ×    ×   ×\n",
      "202601011711     1     3     2     1     3     6    ○    ○    ×   ×\n",
      "202601011712     1     3     2     1     4     6    ○    ×    ×   ×\n",
      "202601012001     1     6     4     4     6     2    ×    ○    ×   ×\n",
      "202601012002     4     2     3     1     6     3    ×    ×    ○   ×\n",
      "202601012003     1     3     2     3     1     4    ×    ×    ×   ×\n",
      "202601012004     1     3     4     1     6     2    ○    ×    ×   ×\n",
      "202601012005     2     1     4     1     2     4    ×    ×    ○   ×\n",
      "202601012006     1     6     2     1     3     5    ○    ×    ×   ×\n",
      "202601012007     2     1     3     2     5     4    ○    ×    ×   ×\n",
      "202601012008     1     3     2     1     2     3    ○    ×    ×   ×\n",
      "202601012009     6     1     4     1     4     3    ×    ×    ×   ×\n",
      "202601012010     1     5     6     3     1     5    ×    ×    ×   ×\n",
      "202601012011     1     2     4     1     4     6    ○    ×    ×   ×\n",
      "202601012012     1     2     3     1     4     3    ○    ×    ○   ×\n",
      "202601012101     1     3     2     1     3     4    ○    ○    ×   ×\n",
      "202601012102     1     2     5     1     4     3    ○    ×    ×   ×\n",
      "202601012103     1     5     2     4     1     2    ×    ×    ○   ×\n",
      "202601012104     4     1     2     3     1     4    ×    ○    ×   ×\n",
      "202601012105     1     3     2     1     2     5    ○    ×    ×   ×\n",
      "202601012106     1     2     4     2     4     3    ×    ×    ×   ×\n",
      "202601012109     1     4     3     1     3     2    ○    ×    ×   ×\n",
      "202601012111     3     1     2     1     3     5    ×    ×    ×   ×\n",
      "202601012112     1     5     2     1     2     3    ○    ×    ×   ×\n"
     ]
    }
   ],
   "source": [
    "# Create detailed comparison results in the same format as confirmation file\n",
    "results_list = []\n",
    "\n",
    "for race_code in sorted(race_actual_sanrentan.keys()):\n",
    "    if race_code in race_predictions:\n",
    "        predicted = race_predictions[race_code]\n",
    "        actual = race_actual_sanrentan[race_code]\n",
    "        \n",
    "        # Extract each position\n",
    "        pred_1st, pred_2nd, pred_3rd = predicted[0], predicted[1], predicted[2]\n",
    "        actual_1st, actual_2nd, actual_3rd = actual[0], actual[1], actual[2]\n",
    "        \n",
    "        # Check matches (○ or ×)\n",
    "        match_1st = '○' if pred_1st == actual_1st else '×'\n",
    "        match_2nd = '○' if pred_2nd == actual_2nd else '×'\n",
    "        match_3rd = '○' if pred_3rd == actual_3rd else '×'\n",
    "        match_all = '○' if (pred_1st == actual_1st and pred_2nd == actual_2nd and pred_3rd == actual_3rd) else '×'\n",
    "        \n",
    "        results_list.append({\n",
    "            'レースコード': race_code,\n",
    "            '予想1着': pred_1st,\n",
    "            '予想2着': pred_2nd,\n",
    "            '予想3着': pred_3rd,\n",
    "            '実際1着': actual_1st,\n",
    "            '実際2着': actual_2nd,\n",
    "            '実際3着': actual_3rd,\n",
    "            '1着的中': match_1st,\n",
    "            '2着的中': match_2nd,\n",
    "            '3着的中': match_3rd,\n",
    "            '全的中': match_all\n",
    "        })\n",
    "\n",
    "if results_list:\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_races = len(results_df)\n",
    "    match_1st_count = (results_df['1着的中'] == '○').sum()\n",
    "    match_2nd_count = (results_df['2着的中'] == '○').sum()\n",
    "    match_3rd_count = (results_df['3着的中'] == '○').sum()\n",
    "    match_all_count = (results_df['全的中'] == '○').sum()\n",
    "    \n",
    "    print('=== 的中率レポート ===')\n",
    "    print(f'レース数: {total_races}')\n",
    "    print(f'1着的中: {match_1st_count}/{total_races} ({match_1st_count/total_races:.1%})')\n",
    "    print(f'2着的中: {match_2nd_count}/{total_races} ({match_2nd_count/total_races:.1%})')\n",
    "    print(f'3着的中: {match_3rd_count}/{total_races} ({match_3rd_count/total_races:.1%})')\n",
    "    print(f'三連単的中: {match_all_count}/{total_races} ({match_all_count/total_races:.1%})')\n",
    "    \n",
    "    print(f'\\n=== 詳細結果 ===')\n",
    "    print(results_df.to_string(index=False))\n",
    "else:\n",
    "    print('No results to display')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
